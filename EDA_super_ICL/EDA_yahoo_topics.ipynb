{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vrYtljvE59Ek"
      },
      "source": [
        "# EDA on a Hugging Face dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fine-tuning the best Large Language Models (LLMs) is not an option: they are proprietary, accessible only via apps and APIs. Fine-tuning an open-source LLM is possible but not always practical or appealing. Thus, In-Context Learning (ICL) is a popular alternative, for those with the right custom data.\n",
        "\n",
        "Background: In-Context Learning (few-shot learning) involves augmentation at inference. The sequence ingested by the model is the unlabeled input concatenated with context (labeled examples) that primes the model toward desired output, albeit indirectly. In 2023, ICL is a hot topic.\n",
        "\n",
        "Challenge: Methods abound for selecting contextual exemplars per input, and the performance benefits of ICL vary dramatically across selection methods. New learning-based approaches show promise ([Ye et al.](https://arxiv.org/abs/2302.05698), [Xu et al.](https://arxiv.org/abs/2305.08848)), but require more labeled examples than do learning-free approaches. Other factors that moderate the influence of contextual subsets include relevance and diversity.\n",
        "\n",
        "Dataset: In 2015, [Zhang et al.](https://arxiv.org/abs/1509.01626) compiled several datasets, including one sourced from Yahoo that is available via Hugging Face. It includes 1.46 million question-answer pairs, labeled according to topic. The content comes from Yahoo Answers, a crowdsourced QA site deprecated in 2021 (content is from pre-2007).\n",
        "\n",
        "Goal: The present EDA will shed light on the suitability of this dataset for learning-based ICL methods, especially SuperICL and CEIL, in support of LLM-based generative QA as well as text classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94nbC_4H59El"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from eda_funcs import *\n",
        "\n",
        "pd.set_option('max_colwidth', 200)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHpVtAcc59Em"
      },
      "source": [
        "## Check size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sh565PS459Em"
      },
      "outputs": [],
      "source": [
        "builder = datasets.load_dataset_builder('yahoo_answers_topics')\n",
        "show_size(builder)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download, preview sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = load_yahoo(split='train', num_shards=32, shard_index=0)\n",
        "print(f\"number of rows: {ds.num_rows}\\n\")\n",
        "ds[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds.cleanup_cache_files()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check topic balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds[:].value_counts(['label', 'topic']).to_frame().sort_index()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17Vl9wh759Eq"
      },
      "outputs": [],
      "source": [
        "ds[:].query(\"answer == ''\").groupby(['topic'])['id'].count().plot(\n",
        "    kind='barh', figsize=(4,3), title='Missing answers')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inspect concise questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfsQv54y59Eq"
      },
      "outputs": [],
      "source": [
        "mono_q = ds.filter(lambda x: len(x['question'].split())==1)[:10]['question']\n",
        "duo_q = ds.filter(lambda x: len(x['question'].split())==2)[:10]['question']\n",
        "\n",
        "pd.DataFrame({'1-word Questions': mono_q, \n",
        "              '2-word Questions': duo_q})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Drop rows with blank answers, blank questions, or 1-word questions; Review topic balance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = ds.filter(lambda x: x['answer'] != '' and x['question'] != '' and len(x['question'].split()) > 1)\n",
        "df = ds[:].value_counts(['label', 'topic']).to_frame().sort_index()\n",
        "df.plot(kind='barh', figsize=(5, 3), title='Examples per Topic', legend=False)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preliminary observations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gxyWo1cl59Er"
      },
      "source": [
        "- This shard of the training dataset is large and balanced. For comparison, Rubin et al. use 44k examples to train a receiver for ICL. This Yahoo Answers dataset from huggingface is 32x that size.\n",
        "- As stated [elsewhere](https://en.wikipedia.org/wiki/Yahoo!_Answers), at least some examples appear silly, inarticulate, or worse."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "___"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bAW-EM2t59Ew"
      },
      "source": [
        "## Check question quality"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How do questions begin?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds.reset_format()\n",
        "ds = ds.map(q_start)\n",
        "ds.set_format('pandas')\n",
        "plot_question_starters(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1trGxQo59Ew"
      },
      "outputs": [],
      "source": [
        "ds[:].query(\"q_start == 'i'\")[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = ds.remove_columns(['q_start'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- *What?* is common.\n",
        "- *Who?* is especially common in Sports, Entertainment & Music.\n",
        "- *Why?* is especially common in Politics & Gov't, Society & Culture.\n",
        "- *How?* is especially common in Computers & Internet.\n",
        "- *I* is unexpectedly common across all topics, framing questions with first-person narrative.\n",
        "\n",
        "Questions that start with *I* are indirect and long, requiring sythesis across sentences and interpretation. These are not the best candidates for ICL."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "___"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a5MHpbZ859Er"
      },
      "source": [
        "## Word counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI6WFuu_59Ex"
      },
      "outputs": [],
      "source": [
        "ds = word_counts(ds)\n",
        "ds[:][['q_word_count', 'ans_word_count']].describe().astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hWyfoeD59Ey"
      },
      "outputs": [],
      "source": [
        "ds.set_format('pandas')\n",
        "plt.figure(figsize=(5, 3))\n",
        "plt.title('Question lengths')\n",
        "plt.xlabel('Number of words in question')\n",
        "plt.ylabel('Frequency')\n",
        "plt.hist(ds['q_word_count'], bins=40, range=(0, 40), histtype='bar', rwidth=2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vysLFUB359Ey"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 3))\n",
        "plt.title('Answer lengths')\n",
        "plt.xlabel('Number of words in answer')\n",
        "plt.ylabel('Frequency')\n",
        "plt.hist(ds[:]['ans_word_count'], bins=80, range=(0,400), histtype='bar', rwidth=2)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = ds.remove_columns(['q_word_count', 'ans_word_count'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- RoBERTa has max sequence length of 512 tokens. The majority of these examples would fit.\n",
        "- The proprietary LLMs have context length ranging up to 4096. Multiple of these examples could fit within a single context."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CDfyuLfJ59Ey"
      },
      "source": [
        "___"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EDA does not determine how a dataset for learning-based ICL would influence a model's output. Even indicators of similarity (between an unlabeled input and a labeled contextual exemplar) and diversity (within the concatenated exemplar set per input) do not determine precisely how well a dataset will work for ICL: the best trade-off between relevance and diversity differs across tasks. The same goes for the number of in-context examples per input. EDA is only a start.\n",
        "\n",
        "Nonetheless, we can draw a few conclusions.\n",
        "- The size and breadth of the Yahoo Answers dataset is its strength, especially for learning-based ICL methods.\n",
        "- The sequence length of tokenized question-answer pairs in this dataset is an appropriate length for the in-context learning methods discussed at the outset, namely CEIL (optimizing the context retriever's subset selection) and SuperICL (fine-tuning RoBERTa in a cascade design).\n",
        "\n",
        "[Rubin et al.](https://aclanthology.org/2022.naacl-main.191/) argue that the best context for ICL is generated by a scoring LLM, separate from the inference LLM. That may be the case. For crowdsourced data, Yahoo Answers provides a large, diverse set.\n",
        "\n",
        "Personally, I would not twist anyone's arm to use this dataset.\n",
        "- It is difficult to fact check.\n",
        "- Other datasets exist -- synthetic or real.\n",
        "- Starting with a small, high-quality dataset seems more reasonable than harvesting a huge, dubious one.\n",
        "- Yahoo shut down the site. Of course, that was in 2021. Still, the decision raises questions."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
